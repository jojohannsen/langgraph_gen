{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ce13c3-0ef7-4c56-ab85-f9efbfc15b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if the notebook is running in Google Colab\n",
    "def in_colab():\n",
    "    return 'google.colab' in sys.modules\n",
    "\n",
    "# If in Colab, create the directory structure and download the required files\n",
    "if in_colab():\n",
    "    os.makedirs('graph_gen', exist_ok=True)\n",
    "    !wget -q https://raw.githubusercontent.com/jojohannsen/langgraph_gen/main/graph_gen/gen_graph.py -O graph_gen/gen_graph.py\n",
    "    !wget -q https://raw.githubusercontent.com/jojohannsen/langgraph_gen/main/graph_gen/__init__.py -O graph_gen/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673dab55-02ab-418d-85d6-5c100f9c77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_gen.gen_graph import gen_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "# How to wait for user input\n",
    "\n",
    "One of the main human-in-the-loop interaction patterns is waiting for human input. A key use case involves asking the user clarifying questions. One way to accomplish this is simply go to the END node and exit the graph. Then, any user response comes back in as fresh invocation of the graph. This is basically just creating a chatbot architecture.\n",
    "\n",
    "The issue with this is it is tough to resume back in a particular point in the graph. Often times the agent is halfway through some process, and just needs a bit of a user input. Although it is possible to design your graph in such a way where you have a `conditional_entry_point` to route user messages back to the right place, that is not super scalable (as it essentially involves having a routing function that can end up almost anywhere).\n",
    "\n",
    "A separate way to do this is to have a node explicitly for getting user input. This is easy to implement in a notebook setting - you just put an `input()` call in the node. But that isn't exactly production ready.\n",
    "\n",
    "Luckily, LangGraph makes it possible to do similar things in a production way. The basic idea is:\n",
    "\n",
    "- Set up a node that represents human input. This can have specific incoming/outgoing edges (as you desire). There shouldn't actually be any logic inside this node.\n",
    "- Add a breakpoint before the node. This will stop the graph before this node executes (which is good, because there's no real logic in it anyways)\n",
    "- Use `.update_state` to update the state of the graph. Pass in whatever human response you get. The key here is to use the `as_node` parameter to apply this update **as if you were that node**. This will have the effect of making it so that when you resume execution next it resumes as if that node just acted, and not from the beginning.\n",
    "\n",
    "**Note:** this requires passing in a checkpointer.\n",
    "\n",
    "Below is a quick example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for Anthropic (the LLM we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e25aec-7c9f-4a63-b143-225d0e9a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "_set_env(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f89e5",
   "metadata": {},
   "source": [
    "## Build the agent\n",
    "\n",
    "We can now build the agent. We will build a relatively simple ReAct-style agent that does tool calling. We will use Anthropic's models and a fake tool (just for demo purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5319e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the state\n",
    "from langgraph.graph import MessagesState, START\n",
    "\n",
    "# Set up the tool\n",
    "# We will have one real tool - a search tool\n",
    "# We'll also have one \"fake\" tool - a \"ask_human\" tool\n",
    "# Here we define any ACTUAL tools\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though ðŸ˜Š\n",
    "    return [\n",
    "        f\"I looked up: {query}. Result: It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\"\n",
    "    ]\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Set up the model\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "# We are going \"bind\" all tools to the model\n",
    "# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n",
    "# Since `bind_tools` takes in tools but also just tool definitions,\n",
    "# We can define a tool definition for `ask_human`\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "    question: str\n",
    "\n",
    "model = model.bind_tools(tools + [AskHuman])\n",
    "\n",
    "# Define nodes and conditional edges\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Build the graph\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# don't really need this when human is node\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c5bab0-b6da-4f19-bd32-2df7d1a4a843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait_user_input = StateGraph(MessagesState)\n",
      "wait_user_input.add_node('call_model', call_model)\n",
      "wait_user_input.add_node('tool_node', tool_node)\n",
      "wait_user_input.add_node('get_human_input', get_human_input)\n",
      "\n",
      "wait_user_input.set_entry_point('call_model')\n",
      "\n",
      "def after_call_model(state: MessagesState):\n",
      "    if no_tools(state):\n",
      "        return 'END'\n",
      "    elif human_needed(state):\n",
      "        return 'get_human_input'\n",
      "    return 'tool_node'\n",
      "\n",
      "call_model_dict = {'END': END, 'get_human_input': 'get_human_input', 'tool_node': 'tool_node'}\n",
      "wait_user_input.add_conditional_edges('call_model', after_call_model, call_model_dict)\n",
      "\n",
      "\n",
      "wait_user_input.add_edge('tool_node', 'call_model')\n",
      "\n",
      "\n",
      "wait_user_input.add_edge('get_human_input', 'call_model')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wait_user_input = wait_user_input.compile(checkpointer=memory)\n"
     ]
    }
   ],
   "source": [
    "def no_tools(state):\n",
    "    return not state[\"messages\"][-1].tool_calls\n",
    "\n",
    "def human_needed(state):\n",
    "    return state[\"messages\"][-1].tool_calls[0][\"name\"] == \"AskHuman\"\n",
    "\n",
    "# the human node\n",
    "def get_human_input(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_call_id = last_message.tool_calls[0][\"id\"]\n",
    "    tool_message = last_message.tool_calls[0]\n",
    "    question = tool_message['args']['question']\n",
    "    weather_place = input(question)\n",
    "    tool_message = [\n",
    "        {\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": weather_place}\n",
    "    ]\n",
    "    return { \"messages\": tool_message }\n",
    "\n",
    "graph_spec = \"\"\"\n",
    "\n",
    "call_model(MessagesState)\n",
    "   no_tools => END\n",
    "   human_needed => get_human_input\n",
    "   => tool_node\n",
    "\n",
    "tool_node\n",
    "   => call_model\n",
    "   \n",
    "get_human_input\n",
    "   => call_model\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "graph_code = gen_graph(\"wait_user_input\", graph_spec, 'memory')\n",
    "print(graph_code)\n",
    "exec(graph_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef09e5c-7508-4bff-b6f7-f3b401ad18ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Use the search tool to ask the user where they are, then look up the weather there\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"Certainly! I'll use the AskHuman tool to ask the user where they are, and then I'll use the search tool to look up the weather in that location. Let's start by asking the user about their location.\", 'type': 'text'}, {'id': 'toolu_01K1ojwrEMeTRMcz59JswSZJ', 'input': {'question': 'Where are you currently located?'}, 'name': 'AskHuman', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  AskHuman (toolu_01K1ojwrEMeTRMcz59JswSZJ)\n",
      " Call ID: toolu_01K1ojwrEMeTRMcz59JswSZJ\n",
      "  Args:\n",
      "    question: Where are you currently located?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Where are you currently located? everywhere\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "everywhere\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': 'I see that the user responded with \"everywhere.\" Since this is not a specific location, we\\'ll need to ask for a more precise answer to be able to look up the weather. Let\\'s ask for a specific city or location.', 'type': 'text'}, {'id': 'toolu_01GwNW7sjAGjTJP4T34sWcrz', 'input': {'question': 'I understand you said \"everywhere,\" but to look up the weather, I need a specific location. Could you please provide a city or town name?'}, 'name': 'AskHuman', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  AskHuman (toolu_01GwNW7sjAGjTJP4T34sWcrz)\n",
      " Call ID: toolu_01GwNW7sjAGjTJP4T34sWcrz\n",
      "  Args:\n",
      "    question: I understand you said \"everywhere,\" but to look up the weather, I need a specific location. Could you please provide a city or town name?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I understand you said \"everywhere,\" but to look up the weather, I need a specific location. Could you please provide a city or town name? yuma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "yuma\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"Thank you for providing a specific location. Now that we know the user is in Yuma, let's use the search tool to look up the weather there.\", 'type': 'text'}, {'id': 'toolu_012vWV12PNbWHGC829jBka6z', 'input': {'query': 'current weather in Yuma'}, 'name': 'search', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  search (toolu_012vWV12PNbWHGC829jBka6z)\n",
      " Call ID: toolu_012vWV12PNbWHGC829jBka6z\n",
      "  Args:\n",
      "    query: current weather in Yuma\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search\n",
      "\n",
      "[\"I looked up: current weather in Yuma. Result: It's sunny in San Francisco, but you better look out if you're a Gemini \\ud83d\\ude08.\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I apologize, but it seems that the search results didn't provide accurate information about the weather in Yuma. Instead, it gave an unrelated response about San Francisco and astrological signs. Let's try a more specific search query to get the weather information for Yuma.\", 'type': 'text'}, {'id': 'toolu_01DBbxkFvecvQLhxGaAJEqC7', 'input': {'query': 'current temperature and conditions in Yuma, Arizona'}, 'name': 'search', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  search (toolu_01DBbxkFvecvQLhxGaAJEqC7)\n",
      " Call ID: toolu_01DBbxkFvecvQLhxGaAJEqC7\n",
      "  Args:\n",
      "    query: current temperature and conditions in Yuma, Arizona\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: search\n",
      "\n",
      "[\"I looked up: current temperature and conditions in Yuma, Arizona. Result: It's sunny in San Francisco, but you better look out if you're a Gemini \\ud83d\\ude08.\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'text': \"I apologize again, but it seems that the search tool is not providing accurate or relevant information about the weather in Yuma, Arizona. Instead, it's returning the same unrelated response about San Francisco and astrological signs. \\n\\nGiven this situation, I think it's best to inform the user about the issue we're experiencing with getting accurate weather information.\", 'type': 'text'}, {'id': 'toolu_01NfcfDJ9ETiKC9ispzU4XdB', 'input': {'question': \"I'm sorry, but I'm having trouble getting accurate weather information for Yuma through the search tool. The results are not relevant to your location. Would you like me to try searching for general information about Yuma instead, or do you have another question I can help you with?\"}, 'name': 'AskHuman', 'type': 'tool_use'}]\n",
      "Tool Calls:\n",
      "  AskHuman (toolu_01NfcfDJ9ETiKC9ispzU4XdB)\n",
      " Call ID: toolu_01NfcfDJ9ETiKC9ispzU4XdB\n",
      "  Args:\n",
      "    question: I'm sorry, but I'm having trouble getting accurate weather information for Yuma through the search tool. The results are not relevant to your location. Would you like me to try searching for general information about Yuma instead, or do you have another question I can help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I'm having trouble getting accurate weather information for Yuma through the search tool. The results are not relevant to your location. Would you like me to try searching for general information about Yuma instead, or do you have another question I can help you with? no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "no\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I understand. Since you've answered \"no\" to my question about searching for general information about Yuma or helping with another question, I'll wrap up our conversation here. \n",
      "\n",
      "To summarize:\n",
      "1. We determined that you're located in Yuma.\n",
      "2. We attempted to search for weather information in Yuma, but encountered issues with getting accurate results.\n",
      "3. You declined further assistance or alternative searches.\n",
      "\n",
      "Is there anything else you'd like to know or discuss? If not, feel free to ask if you have any other questions in the future. I'm here to help!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "input_message = HumanMessage(\n",
    "    content=\"Use the search tool to ask the user where they are, then look up the weather there\"\n",
    ")\n",
    "for event in wait_user_input.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f972d1-3d99-4fc1-8b33-92b71e74835d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
